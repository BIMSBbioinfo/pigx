#!/@PYTHON@

# PIGx BSseq Pipeline.
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program.  If not, see <http://www.gnu.org/licenses/>.




import argparse

description = """\
PIGx BSseq Pipeline.

PIGx is a data processing pipeline for raw fastq read data of
bisulfite experiments.  It produces methylation and coverage
information and can be used to produce information on differential
methylation and segmentation.
"""

epilog = 'This pipeline was developed by the Akalin group at MDC in Berlin in 2017.'

version = """\
PIGx BSseq Pipeline.
Version: @PACKAGE_VERSION@

Copyright Â© 2017 Alexander Gosdschan, Katarzyna Wreczycka, Bren Osberg, Ricardo Wurmus.
License GPLv3+: GNU GPL version 3 or later <http://gnu.org/licenses/gpl.html>.

This is free software: you are free to change and redistribute it.
There is NO WARRANTY, to the extent permitted by law.
"""

def formatter(prog):
    return argparse.RawTextHelpFormatter(prog, max_help_position=80)

parser = argparse.ArgumentParser(description=description,
                                 epilog=epilog,
                                 formatter_class=formatter)

parser.add_argument('-v', '--version', action='version',
                    version=version)

parser.add_argument('-t', '--tablesheet', dest='tablesheet',
                    help="""\
The tablesheet containing the basic configuration information for
running the pipeline.\
""")

parser.add_argument('-p', '--programs', dest='programs',
                    help='A JSON file containing the absolute paths of the required tools.')

parser.add_argument('-c', '--configfile', dest='configfile', default='./config.json',
                    help="""\
The config file used for calling the underlying snakemake process.  By
default the file 'config.json' is dynamically created from tablesheet
and programs file.
""")

parser.add_argument('-s', '--snakeparams', dest='snakeparams', default='',
                    help="""\
Additional parameters to be passed down to snakemake, e.g.
    --dryrun    do not execute anything
    --forceall  re-run the whole pipeline""")

args = parser.parse_args()



# Generate config file

# Only generate the config file if it does not exist, or if the
# tablesheet or progs files have changed.  The hashes of these files
# are stored and compared to determine changes.

from os import path
import os, sys, json, hashlib, csv

def bail(msg):
    """Print the error message to stderr and exit."""
    print(msg, file=sys.stderr)
    exit(1)

def file_hash(fname):
    """Return the SHA256 hash of the given file FNAME."""
    return hashlib.sha256(open(fname, 'rb').read()).hexdigest()

def config_hashfile(configfile):
    return path.join(path.dirname(configfile), '.pigx.hashes')

def dump_hashes(target, tablesheet, programs):
    """Write the hashes of the TABLESHEET and PROGRAMS files to TARGET."""
    data = {
        'tablesheet': {
            'sha256': file_hash(tablesheet),
            'file'  : tablesheet
        },
        'programs': {
            'sha256': file_hash(programs),
            'file'  : programs
        }
    }
    open(target, 'w').write(json.dumps(data))

def parse_table_sheet(file):
  """ Parse the tablesheet given in FILE and return its sections."""
  sections = {}
  section = None
  with open(file) as f:
      for line in f:
          if line.startswith("["):
              section = line.strip('[]\n ').lower()
              sections[section] = list()
          else:
              line = line.strip()
              if not section and line:
                  print("WARNING: ignoring line outside of section.")
              if section and line:
                  sections[section].append(line)
  return sections

def parse_assignments(lines):
    """Parse LINES of assignments in the form key='value'.  Return a
    dictionary in which keys are variables (e.g. PATHIN) and values are
    given by a user."""
    entries = [x.rstrip().replace('"', "").split("=") for x in lines]
    params = {}
    for key, value in entries:
        params[key] = value
    return params

def parse_diff_meth(lines, sample_params):
    """
    Parse lines with information pairs samples
    which should be considered for differential
    methylation. E.g. for treatments A,B,C and D
    it could like:
  
    A, B
    C, D
    D, A, B
  
    It returns a dictionary required for the config file
    with a list of treatment values, e.g:
    [[A,B], [C,D], [D,A,B]]
    """
    # remove \n characters after arguments
    text = [line.rstrip() for line in lines]
    # remove empty lines
    text = list(filter(None, text))

    # list of lists (variable, value)
    list_of_list =[[field.strip() for field in line.split(",")] for line in text]

    # check if treatment values are in the the column samples from
    # the second part of the input file
    # treatments values from the [[ SAMPLES ]] part
    treatments_samples = set([sample_params['SAMPLES'][s]['Treatment'] 
                              for s in sample_params['SAMPLES'].keys()])

    # treatments values from the [[ DIFFERENTIAL METHYLATION ]] part
    treatments_diffmeth = set(sum(list_of_list, []))
    if not treatments_diffmeth.issubset(treatments_samples):
        invalid_treatments = list(set(treatments_diffmeth) - set(treatments_samples))
        raise Exception("Invalid treatment value(s) " + ", ".join(invalid_treatments))

    return {"DIFF_METH": list_of_list}

def get_filenames(mylist):
    return list(map(lambda x: splitext_fqgz(x)[0], mylist))

def get_extension(mylist):
    return list(map(lambda x: splitext_fqgz(x)[1], mylist))

def fq_suffix(filename):
    return any(filename.endswith(ext) for ext in [".fq", ".fastq", ".fasta"])

def is_zipped(filename):
    return any(filename.endswith(ext) for ext in [".gz", ".bz2"])

def splitext_fqgz(string):
    if is_zipped(string):
        string, zipext = os.path.splitext(string)
    else:
        zipext = ""
    if fq_suffix(string):
        base, ext = os.path.splitext(string)
        return (base, ext + zipext)
    else:
        bail("Input files are not fastq files!")

def parse_samples(lines):
    """
    Parse csv table with information about samples, eg:
    
    Read1,Read2,SampleID,ReadType,Treatment
    sampleB.pe1.fq.gz,sampleB.pe2.fq.gz,sampleB,WGBS,B,,
    pe1.single.fq.gz,,sampleB1,WGBS,B,,
    
    It returns a dictionary required for the config file.
    """
    sreader = csv.reader(lines, delimiter=',')
    all_rows = [row for row in sreader]
  
    header = all_rows[0]
    rows   = all_rows[1:]
    minimal_header = ['Read1', 'Read2', 'SampleID', 'ReadType', 'Treatment']
  
    if header[:5] != minimal_header:
        raise Exception("First columns of the input table have to be " +
                        ",".join(minimal_header) + ".")

    sample_ids = [x[2] for x in rows]
    if len(set(sample_ids)) != len(sample_ids):
        raise Exception("Column 'SampleID' has non-unique values.")

    # Create a dictionary with all params, keys are samples ids
    outputdict = {}
    for row in rows:
        files = list(filter(None, row[0:2]))
        if not files:
            raise Exception("Each sample has to have an entry in at least one of the columns 'Read1' or 'Read2'.")
      
        sampleid_dict = {}
        for idx in range(len(header[2:])):
            try:
                sampleid_dict[header[2:][idx]] = row[2:][idx]
            except IndexError:
                raise Exception("Number of columns in row " + idx + " doesn't match number of elements in header.")

        sampleid_dict['files']      = files
        sampleid_dict['fastq_name'] = get_filenames(files)
        sampleid_dict['fastq_ext']  = get_extension(files)
        outputdict[row[2]] = sampleid_dict
    return { 'SAMPLES': outputdict }

def generate_config(configfile, tablesheet, progsfile):
    """Generate a new configuration file CONFIG using TABLESHEET and
PROGSFILE as inputs."""
    sections = parse_table_sheet(tablesheet)
  
    # Load general parameters
    gen_params = parse_assignments(sections['general parameters'])

    # Load parameters specific to samples
    sample_params = parse_samples(sections['samples'])

    # Load pairs of treatments for differential methylation
    diff_meth = sections['differential methylation']
    diff_meth_params = parse_diff_meth(diff_meth, sample_params)

    # Load programs
    progs = {'programs': dict(json.load(open(progsfile, 'r')))}

    dirs = {}
    if os.getenv('PIGX_BSSEQ_UNINSTALLED'):
        here = os.getcwd()
        dirs['dirs'] = {
            'prefix'       : here,
            'exec_prefix'  : here,
            'libexecdir'   : here,
            'pkglibexecdir': here,
            'datarootdir'  : here,
            'pkgdatadir'   : here
        }
    else:
        # Expand and store autoconf directory variables
        prefix = '@prefix@'
        exec_prefix = '@exec_prefix@'[1:].format(prefix=prefix)
        libexecdir = '@libexecdir@'[1:].format(exec_prefix=exec_prefix)
        pkglibexecdir = '{libexecdir}/@PACKAGE@'.format(libexecdir=libexecdir)
        datarootdir = '@datarootdir@'[1:].format(prefix=prefix)
        pkgdatadir = '@datadir@/@PACKAGE@'[1:].format(datarootdir=datarootdir)

        dirs['dirs'] = {
            'prefix'       : '@prefix@',
            'exec_prefix'  : exec_prefix,
            'libexecdir'   : libexecdir,
            'pkglibexecdir': pkglibexecdir,
            'datarootdir'  : datarootdir,
            'pkgdatadir'   : pkgdatadir
        }
  
    # Create a config file
    config_dict = gen_params
    config_dict.update(sample_params)
    config_dict.update(diff_meth_params)
    config_dict.update(progs)
    config_dict.update(dirs)

    # Write the config file
    with open(configfile, 'w') as outfile:
        dumps = json.dumps(config_dict,
                           indent=4, sort_keys=True,
                           separators=(",",": "), ensure_ascii=True)
        outfile.write(dumps)
    dump_hashes(config_hashfile(configfile), tablesheet, progsfile)

def config_fresh(configfile):
    """Return a tuple of whether the CONFIG is stale and the parsed JSON
containing the input file names and hashes."""
    hashfile = config_hashfile(configfile)
    if not (path.isfile(hashfile) and path.isfile(configfile)):
        return (False, None)
    hashes = json.load(open(hashfile, 'r'))
    if ((hashes['tablesheet']['sha256'] == file_hash(hashes['tablesheet']['file'])) and
        (hashes['programs']['sha256'] == file_hash(hashes['programs']['file']))):
        return (True, hashes)
    else:
        return (False, hashes)


fresh, hashes = config_fresh(args.configfile)
if not fresh:
    if hashes:
        generate_config(args.configfile,
                        hashes['tablesheet']['file'],
                        hashes['programs']['file'])
    else:
        if not args.tablesheet:
            bail('Must provide tablesheet file.')
        if args.programs:
            programs = args.programs
        else:
            if os.getenv('PIGX_BSSEQ_UNINSTALLED'):
                programs = './etc/programs.json'
            else:
                # Expand autoconf variables
                datarootdir = '@datarootdir@'[1:].format(prefix='@prefix@')
                pkgdatadir = '@datadir@/@PACKAGE@'[1:].format(datarootdir=datarootdir)
                programs = '{pkgdatadir}/programs.json'.format(pkgdatadir=pkgdatadir)

        generate_config(args.configfile,
                        args.tablesheet,
                        programs)

config = json.load(open(args.configfile, 'r'))


# Create symbolic links to the inputs and reference genome

# Create links within the output folder that point directly to the
# reference genome, as well as to each sample input file so that it's
# clear where the source data came from.

# N.B. Any previously existing links will be kept in place, and no
# warning will be issued if this is the case.

def makelink(src, target):
    if not path.isfile(src):
        bail("Refusing to link non-existent file %s" % src)
    elif not path.isdir(path.dirname(target)):
        bail("%s or subdirectory does not exist for linking %s" % config['PATHOUT'], target)
    else:
        try:
            os.symlink(src, target)
        except FileExistsError:
            pass

os.makedirs(path.join(config['PATHOUT'], 'path_links/input'),
            exist_ok=True)

# Link the reference genome
try:
    os.symlink(config['GENOMEPATH'],
               path.join(config['PATHOUT'], 'path_links/refGenome'))
except FileExistsError:
    pass

# Create file links
for sample in config['SAMPLES']:
  flist = config['SAMPLES'][sample]['files']
  single_end = len(flist) == 1

  for idx, f in enumerate(flist):
    if not f.endswith(".gz"):
      # FIXME: Future versions should handle unzipped .fq or .bz2.
      bail("Input files must be gzipped: %s." % f)

    tag = "" if single_end else '_' + str(idx + 1)
    linkname = config['SAMPLES'][sample]['SampleID'] + tag + ".fq.gz"
    makelink(path.join(config['PATHIN'], f),
             path.join(config['PATHOUT'], "path_links/input/", linkname))



# Run snakemake!

def cluster_run():
    if 'contact_email' in config and not config['contact_email'].lower() == 'none':
        contact_email_string = "-m abe -M %s" % config['contact_email']
    else:
        contact_email_string = ""

    bismark_cores = int(config['bismark_cores'])
    bismark_pe_threads = 4 * bismark_cores
    bismark_se_threads = 2 * bismark_cores

    # Create the cluster configuration file
    rules = [
        ('__default__', 1, config['MEM_default']),
        ('bismark_se', bismark_se_threads, config['bismark_MEM']),
        ('bismark_pe', bismark_pe_threads, config['bismark_MEM'])
    ]

    cluster_conf = {}
    for rule, nthreads, mem in rules:
        cluster_conf[rule] = {
            'nthreads': nthreads,
            'q': config['qname'],
            'MEM': mem,
            'h_stack': config['h_stack']
        }

    cluster_config_file = "cluster_conf.json"
    open(cluster_config_file, 'w').write(json.dumps(cluster_conf))

    print("Commencing snakemake run submission to cluster")
    qsub = "qsub -V -l h_stack={cluster.h_stack}  -l h_vmem={cluster.MEM} %s -b y -pe smp {cluster.nthreads} -cwd" % contact_email_string
    os.system('@SNAKEMAKE@ -s {}/BSseq_pipeline.py --configfile {} --cluster-config {} -d {} --cluster "{}" --jobs {} {}'
              .format(config['dirs']['pkglibexecdir'],
                      args.configfile,
                      cluster_config_file,
                      config['PATHOUT'],
                      qsub,
                      config['numjobs'],
                      args.snakeparams))


if 'cluster_run' in config and not config['cluster_run'].lower() == 'false':
    cluster_run()
else:
    print("Commencing snakemake run submission locally")
    os.system('@SNAKEMAKE@ -s {}/BSseq_pipeline.py --configfile {} -d {} --jobs {} {}'
              .format(config['dirs']['pkglibexecdir'],
                      args.configfile,
                      config['PATHOUT'],
                      config['numjobs'],
                      args.snakeparams))
